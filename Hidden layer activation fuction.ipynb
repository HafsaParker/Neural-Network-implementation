{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887612cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step function: if your input is greater than 0 output will be 1 and vise-versa\n",
    "Now using this step function as activation function.\n",
    "It is done after imput*weights+bias\n",
    "hidden layer and output layer have different activation func but here we are using step function as activation func.\n",
    "IN realuty we dont use step func but sigmoid function.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a880ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "activation function: if x >0 output is x\n",
    "if x <= 0 output is 0 (rectified linear func becasue its less complicated)\n",
    "Why we use Activation Function?\n",
    "Because if we use simple linear network with like sin wave it cannot fit. maybe for 1 neuron but not for a network\n",
    "but RELu can adjust itself. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec33c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = [[1,2,3,2.5],\n",
    "    [2.0,5.0,-1.0,2.0],\n",
    "    [-1.5,2.7,3.3,-0.8]]\n",
    "Input = [0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "output = []\n",
    "np.random.seed(0)\n",
    "## here we will implement RElu activation\n",
    "for i in Input:\n",
    "    if i>0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "print(output)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e50ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "or we can simply do\n",
    "\"\"\"\n",
    "Output = []\n",
    "for i in Input:\n",
    "    Output.append(max(0,i))\n",
    "print(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1d087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
      "  4.56846210e-05]\n",
      " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
      "  6.10024377e-04]\n",
      " ...\n",
      " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
      "  0.00000000e+00]\n",
      " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
      "  0.00000000e+00]\n",
      " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now lets convert this into object\n",
    "with back pass and forwrd pass\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "##we will also use this for our dataset\n",
    "\n",
    "X = [[1,2,3,2.5],\n",
    "    [2.0,5.0,-1.0,2.0],\n",
    "    [-1.5,2.7,3.3,-0.8]]\n",
    "Input = [0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "X,y = spiral_data(100,3)\n",
    "class layer_Dense:\n",
    "    def __init__(self,n_input,n_neurons):\n",
    "        self.weights = 0.10*np.random.randn(n_input,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "   \n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "class Activation_Relu:\n",
    "    #it will get input and produce an output\n",
    "    def forward(self,Input):\n",
    "        self.output = np.maximum(0,Input)\n",
    "        \n",
    "        \n",
    "        \n",
    "layer1 = layer_Dense(2,5)\n",
    "activation1  = Activation_Relu()\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)\n",
    "##the output will have 0 instead of neg value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Requiremnt \n",
    "##pip install nnfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
