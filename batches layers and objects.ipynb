{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e73223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Batches:\n",
    "1. we can calculate the things in parallel the bigger the batch there will be more parallization. That is\n",
    "why we also use GPU compaired to CPU. Because GPU have more cores to perform fatser calculations.\n",
    "2. Helps with generalization. Rather than telling your machine one feature at a time you can now show multiple.\n",
    "Dont show all of the data because it will cause over fitting.\n",
    "let batch size be: 32 64 128\n",
    "\"\"\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff0f81d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.8   -1.79   1.885]\n",
      " [ 6.9   -4.81  -0.3  ]\n",
      " [-0.59  -1.949 -0.474]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have to perform transpose as matrix multiplication is important\n",
    "1x0.2+2.0x0.8+-1.5x-0.5+?x1.0 --> its a shape error so we can solve this by performing transpose (making rows the columns,vise versa)\n",
    "\"\"\"\n",
    "weights = [[0.2,0.8,-0.5,1.0],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,0.17,0.87]]\n",
    "\"\"\"here as there is only 1 neuron so we wont change or add the weight \"\"\"\n",
    "Input = [[1,2,3,2.5],\n",
    "         [2.0,5.0,-1.0,2.0],\n",
    "         [-1.5,2.7,3.3,-0.8]]\n",
    "bias = [2,3,0.5]\n",
    "##covert the weights to numpy array and transpose\n",
    "output = np.dot(Input,np.array(weights).T)+bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217b563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.8819   5.33225 -3.80535]\n",
      " [ 0.0434   6.2988  -3.1207 ]\n",
      " [-1.01914  2.8397  -1.89101]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "creating more layers now\n",
    "\"\"\"\n",
    "weights = [[0.2,0.8,-0.5,1.0],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "bias = [2,3,0.5]\n",
    "#layer 2\n",
    "weights2 = [[0.1,-0.14,-0.5],\n",
    "           [0.5,0.12,0.33],\n",
    "           [-0.44,-0.73,-0.13]]\n",
    "\n",
    "bias2 = [-1,2,-0.5]\n",
    "output_layer1 = np.dot(Input,np.array(weights).T)+bias ##it will become input for layer 2\n",
    "output_layer2 = np.dot(output_layer1,np.array(weights2).T)+bias2\n",
    "print(output_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb4a634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_output:  [[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "layer2_output [[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now objectifying the layers \n",
    "\"\"\"\n",
    "#This input will be our actaul data now\n",
    "X = [[1,2,3,2.5],\n",
    "    [2.0,5.0,-1.0,2.0],\n",
    "    [-1.5,2.7,3.3,-0.8]]\n",
    "\"\"\"\n",
    "now we will define hidden layer.\n",
    "We will initialize by adding weights randomly.\n",
    "As a progarmmer we should know the number of input and its features.\n",
    "we need our weights to be smaller thus maybe like betweeb -1 to 1 -0.1 to 0.1\n",
    "selecting big number will create bid out put that will not be helpful thus x weight by some small number \n",
    "lets say 0.10.\n",
    "\"\"\"\n",
    "np.random.seed(0)\n",
    "class layer_Dense:\n",
    "    def __init__(self,n_input,n_neurons):\n",
    "        self.weights = 0.10*np.random.randn(n_input,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "    \"\"\"input can be from 2 things from actaul data X or from hidden layer\"\"\"\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights)+self.biases\n",
    " ## size of input and how many neurons we want\n",
    "##here we know the number of input that is features =4\n",
    "layer1 = layer_Dense(4,5)\n",
    "layer2 = layer_Dense(5,2)\n",
    "layer1.forward(X)\n",
    "print(\"layer1_output: \",layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(\"layer2_output\",layer2.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f77efc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
