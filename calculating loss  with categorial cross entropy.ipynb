{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "before optamizing the NN we have to find the matrix of error.\n",
    "Elements of categorial cross entropy :\n",
    "1. classes=3 (will create a vector len of class )\n",
    "2. label=0  (if label =1 one hot = [0,1,0] if label =2 one hot = [0,0,1])\n",
    "3.one hot = [1,0,0]\n",
    "4.prediction= [0.7,0.1,0,2] (probab distribution)\n",
    "thus,\n",
    "using the loss functiom = -(1*log(0.7)+0*log(0.1)+0*log(0.2))\n",
    "this will also simplify our things as many val will be zero.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4887f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "math.log\n",
    "softmax_ouput= [ 0.7,0.1,0.2]\n",
    "target_output = [1,0,0]\n",
    "loss= -(math.log(softmax_ouput[0])*target_output[0]+\n",
    "       math.log(softmax_ouput[1])*target_output[1]+\n",
    "       math.log(softmax_ouput[2])*target_output[2])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the more the confidence the less the error and viseversa.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8a9a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "applying cross entropy.Our losses are:\n",
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Soft_max_output = np.array([[0.7,0.1,0.2],\n",
    "                   [0.1,0.5,0.4],\n",
    "                   [0.02,0.9,0.09]])\n",
    "class_target = [0,1,1]\n",
    "print(Soft_max_output[[0,1,2],class_target])\n",
    "print(\"applying cross entropy.Our losses are:\")\n",
    "print(-np.log(Soft_max_output[range(len(Soft_max_output)),class_target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed202de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 1.098445\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "##we will also use this for our dataset\n",
    "\n",
    "X = [[1,2,3,2.5],\n",
    "    [2.0,5.0,-1.0,2.0],\n",
    "    [-1.5,2.7,3.3,-0.8]]\n",
    "Input = [0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "X,y = spiral_data(100,3)\n",
    "class layer_Dense:\n",
    "    def __init__(self,n_input,n_neurons):\n",
    "        self.weights = 0.10*np.random.randn(n_input,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "   \n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "class Activation_Relu:\n",
    "    #it will get input and produce an output\n",
    "    def forward(self,Input):\n",
    "        self.output = np.maximum(0,Input)\n",
    "        \n",
    "class Softmax_Activation:\n",
    "    def forward(self,Input):\n",
    "        ##this way w will not hit the overflow value\n",
    "        exp_val = np.exp(Input-np.max(Input,axis=1,keepdims=True))\n",
    "        probabilities  = exp_val/np.sum(exp_val,axis = 1 , keepdims=True)\n",
    "        self.output = probabilities\n",
    "class loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_loss = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n",
    "class loss_categorical_cross_entropy(loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7) ##number close to zero bcz otherwise it will give inf error\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_conf = y_pred_clipped[range(samples),y_true]\n",
    "        else:\n",
    "            corrrect_conf = np.sum(y_pred_clipped*y_true,axis=1)\n",
    "        neg_loglikelihood = -np.log(correct_conf)\n",
    "        return neg_loglikelihood\n",
    "\n",
    "        \n",
    "layer1 = layer_Dense(2,3)\n",
    "activation1  = Activation_Relu()\n",
    "layer2 = layer_Dense(3,3)\n",
    "activation2  = Softmax_Activation()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "#print(activation2.output[:5]) #start 5 val of batch\n",
    "\n",
    "loss_func = loss_categorical_cross_entropy()\n",
    "loss = loss_func.calculate(activation2.output,y)\n",
    "print(\"LOSS:\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f0335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
